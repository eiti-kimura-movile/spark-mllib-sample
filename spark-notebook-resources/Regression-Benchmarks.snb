{
  "metadata" : {
    "name" : "Regression-Benchmarks",
    "user_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "auto_save_timestamp" : "1969-12-31T21:00:00.000Z",
    "language_info" : {
      "name" : "scala",
      "file_extension" : "scala",
      "codemirror_mode" : "text/x-scala"
    },
    "trusted" : true,
    "customLocalRepo" : null,
    "customRepos" : null,
    "customDeps" : null,
    "customImports" : null,
    "customArgs" : null,
    "customSparkConf" : null
  },
  "cells" : [ {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.regression.LinearRegressionModel\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\nimport org.apache.spark.mllib.regression.LassoModel\nimport org.apache.spark.mllib.regression.LassoWithSGD\nimport org.apache.spark.mllib.regression.RidgeRegressionModel\nimport org.apache.spark.mllib.regression.RidgeRegressionWithSGD\n\n\nval ROOT_DIR = \"/Users/eiti/spark-ml\"",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "import org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.feature.StandardScaler\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.mllib.regression.LinearRegressionModel\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\nimport org.apache.spark.mllib.regression.LassoModel\nimport org.apache.spark.mllib.regression.LassoWithSGD\nimport org.apache.spark.mllib.regression.RidgeRegressionModel\nimport org.apache.spark.mllib.regression.RidgeRegressionWithSGD\nROOT_DIR: String = /Users/eiti/spark-ml\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "/Users/eiti/spark-ml"
      },
      "output_type" : "execute_result",
      "execution_count" : 1
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "def buildLabelValue(list: List[Double]) : Double = {\n  // index = 4 is the number of success of the hour, that is what we want to predict\n  return if (list(4) != 0.0) Math.log(list(4)) else 0.0\n}\n\ndef buildFeatures(list: List[Double]) : List[Double] = {\n   // remove the index 4, which means the number of success\n   return list.patch(4, Nil, 1)\n}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "buildLabelValue: (list: List[Double])Double\nbuildFeatures: (list: List[Double])List[Double]\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 2
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// reading pre processed dataset\nval rdd = sc.objectFile[List[Double]](ROOT_DIR +\"/rdd-processed\")\n\n// building the LabelPoint, using success as Label\nval labelSet = rdd.map{l => val label = buildLabelValue(l)\n                            val features = buildFeatures(l)\n                            LabeledPoint(label, Vectors.dense(features.toArray))}\n",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "rdd: org.apache.spark.rdd.RDD[List[Double]] = MapPartitionsRDD[1] at objectFile at <console>:71\nlabelSet: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2] at map at <console>:74\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : "MapPartitionsRDD[2] at map at &lt;console&gt;:74"
      },
      "output_type" : "execute_result",
      "execution_count" : 3
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "//Split data into training  and test.\nval splits = labelSet.randomSplit(Array(0.70, 0.30), seed = 13L)\nval training = splits(0)\nval test = splits(1)\n\n//val training = labelSet\n//val test = training\nprintln(\"Training set Label sample:\" + training.take(1).mkString(\"\"))",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "Training set Label sample:(11.02587681986334,[4.0,17.0,3.0,2259.309523809524,2475165.2857142854,94936.71428571429,2631545.714285714])\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[3] at randomSplit at <console>:75, MapPartitionsRDD[4] at randomSplit at <console>:75)\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[3] at randomSplit at <console>:75\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[4] at randomSplit at <console>:75\n"
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 4
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// normalizer function\ndef normTrainingSet(rdd:RDD[LabeledPoint]) : scala.collection.immutable.Map[Double, RDD[LabeledPoint]] = {\n  \n  // StandardScaler for data normalization\n  val scaler = new StandardScaler(withMean = false, withStd = true)\n                   .fit(rdd.map(x => x.features))\n  \n  // split data by carrier id\n  val range = List(1.0, 2.0, 4.0, 5.0)\n  \n  // return (Double, Some(RDD))\n  return range.map{idx => \n                  val trainingSet = rdd.filter(l => l.features.apply(0) == idx)\n                                       .map(x => LabeledPoint(x.label, scaler.transform(x.features)))\n                 (idx, trainingSet)\n            }.toMap\n}\n\n\n// build and filter the training set map\ndef buildTrainingSet(rdd:RDD[LabeledPoint]) : scala.collection.immutable.Map[Double, RDD[LabeledPoint]] = {\n\n  // split data by carrier id\n  val range = List(1.0, 2.0, 4.0, 5.0)\n  \n  // return (Double, Some(RDD))\n  return range.map{idx => \n                  val trainingSet = rdd.filter(l => l.features.apply(0) == idx)\n                 (idx, trainingSet)\n            }.toMap\n}\n\n// build and train model by carrier id\ndef buildSGDModelMap(rdd:scala.collection.immutable.Map[Double, RDD[LabeledPoint]]) : scala.collection.immutable.Map[Double, LinearRegressionModel] = {\n  val range = List(1.0, 2.0, 4.0, 5.0)\n  // return (Double, Some(RDD))\n  return range.map{idx => \n                     // Building the model\n                    val numIterations = 100\n                    var regression = new LinearRegressionWithSGD().setIntercept(true)\n                    regression.optimizer.setStepSize(0.1)\n                    regression.optimizer.setNumIterations(numIterations)\n                     \n                    // get dataset\n                    val dataset = rdd.get(idx).orNull;\n                    if (dataset == null) println(\"ERROR: data set is null for carrier:\" + idx)\n                    (idx, regression.run(dataset))\n                  }.toMap\n}\n\n// build and train model by carrier id\ndef buildRidgeRegressionSGDModelMap(rdd:scala.collection.immutable.Map[Double, RDD[LabeledPoint]]) : scala.collection.immutable.Map[Double, RidgeRegressionModel] = {\n  val range = List(1.0, 2.0, 4.0, 5.0)\n  // return (Double, Some(RDD))\n  return range.map{idx => \n                     // Building the model\n                    val numIterations = 100\n                    var regression = new RidgeRegressionWithSGD().setIntercept(true)\n                    regression.optimizer.setStepSize(0.1)\n                    regression.optimizer.setNumIterations(numIterations)\n                     \n                    // get dataset\n                    val dataset = rdd.get(idx).orNull; \n                    (idx, regression.run(dataset))\n                  }.toMap\n}\n\n// build and train model by carrier id\ndef buildLassoSGDModelMap(rdd:scala.collection.immutable.Map[Double, RDD[LabeledPoint]]) : scala.collection.immutable.Map[Double, LassoModel] = {\n  val range = List(1.0, 2.0, 4.0, 5.0)\n  // return (Double, Some(RDD))\n  return range.map{idx => \n                     // Building the model\n                    val numIterations = 100\n                    var regression = new LassoWithSGD().setIntercept(true)\n                    regression.optimizer.setStepSize(0.1)\n                    regression.optimizer.setNumIterations(numIterations)\n                     \n                    // get dataset\n                    val dataset = rdd.get(idx).orNull; \n                    (idx, regression.run(dataset))\n                  }.toMap\n}\n\n\n// build and train model by carrier id\ndef buildDecTreeModelMap(rdd:RDD[LabeledPoint]) : scala.collection.immutable.Map[Double, DecisionTreeModel] = {\n  val range = List(1.0, 2.0, 4.0, 5.0)\n  \n  val categoricalFeaturesInfo = Map[Int, Int]()\n  val impurity = \"variance\"\n  val maxDepth = 7\n  val maxBins = 32\n  \n  // return (Double, Some(RDD))\n  return range.map{idx => \n                   val filteredSet = training.filter(l => l.features.apply(0) == idx)\n                    // building the model\n                   \n                   val model = DecisionTree.trainRegressor(filteredSet, categoricalFeaturesInfo, impurity, maxDepth, maxBins);\n                  (idx, model)\n                  }.toMap\n}\n\n// prints result of algorithm tested\ndef printStats(computedRdd:RDD[(Double, Double)]) = {\n    val dtTotalCorrect = computedRdd.map{ case (v, p) =>  \n                                       val error = (math.exp(v) - math.exp(p))/math.exp(v);\n                                       if (error > 0.35) 0 else 1;\n                                 }.sum()\n\n    val dtAccuracy = dtTotalCorrect / computedRdd.count\n    val MeanSquaredError = computedRdd.map{ case (v, p) => math.pow(v - p, 2) }.mean()\n    val RootMeanSquaredError = math.sqrt(MeanSquaredError)\n    val MeanAbsoluteError = computedRdd.map{ case (v, p) => math.abs(v - p)/p }.mean()\n\n    println(\"Model Accuracy (ACC) = \" + dtAccuracy)\n    println(\"Mean Squared Error (MSE) = \" + MeanSquaredError)\n    println(\"Root Mean Squared Error (RMSE) = \" + RootMeanSquaredError)\n    println(\"Mean Absolute Error (MAE) = \" + MeanAbsoluteError)\n}",
    "outputs" : [ {
      "name" : "stdout",
      "output_type" : "stream",
      "text" : "normTrainingSet: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])scala.collection.immutable.Map[Double,org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]]\nbuildTrainingSet: (rdd: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])scala.collection.immutable.Map[Double,org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]]\nbuildSGDModelMap: (rdd: scala.collection.immutable.Map[Double,org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]])scala.collection.immutable.Map[Double,org.apache.spark.mllib.regression.LinearRegressionModel]\nbuildRidgeRegressionSGDModelMap: (rdd: scala.collection.immutable.Map[Double,org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]..."
    }, {
      "metadata" : { },
      "data" : {
        "text/html" : ""
      },
      "output_type" : "execute_result",
      "execution_count" : 5
    } ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val mapDecTreeModel = buildDecTreeModelMap(training)\nval mapTraining = normTrainingSet(training)\nval mapTest = normTrainingSet(test)\nval mapSGDModel = buildSGDModelMap(mapTraining)\nval mapLassoSGDModel = buildLassoSGDModelMap(mapTraining)\nval mapRidgeRegressionSGDModel = buildRidgeRegressionSGDModelMap(mapTraining)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "val range = List(1.0, 2.0, 4.0, 5.0)\nrange.map{idx =>\n   println(\"Linear Model with SGD - carrier:\" + idx.toInt + \", weights:\" + mapSGDModel(idx).weights)\n}\n\nrange.map{idx =>\n   println(\"Lasso with SGD Model - carrier:\" + idx.toInt + \", weights:\" + mapLassoSGDModel(idx).weights)\n}\n\nrange.map{idx =>\n   println(\"Ridge Regression with SGD Model - carrier:\" + idx.toInt + \", weights:\" + mapRidgeRegressionSGDModel(idx).weights)\n}\n\nprintln(\"features: \" + labelSet.filter(l => l.features.apply(0) == 1.0).take(1)(0))\nprintln(\"normalized features: \" + mapTraining(1.0).take(1)(0))",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "// persisting mapDecTreeModel models to disk\nmapDecTreeModel.foreach{case (carrier, model) => \n                          val modelName = \"c\" + carrier.toInt + \"model\"\n                          model.save(sc, ROOT_DIR + \"/trained-models/\" + modelName)\n                       }",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// Evaluate model on test dataset\nval computedSet = mapTest.map { case(idx, dataset) =>\n    dataset.map{point => \n                    val model = mapSGDModel(idx)\n                    val prediction = model.predict(point.features)\n                    (point.label, prediction)\n               }.collect\n}\n\nprintln(\"== Linear Model with SGD ==\")\nval predictionsSGD = sc.parallelize(computedSet.reduce(_++_))\nprintStats(predictionsSGD)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// Evaluate model on test dataset\nval computedSetLasso = mapTest.map { case(idx, dataset) =>\n    dataset.map{point => \n                    val model = mapLassoSGDModel(idx)\n                    val prediction = model.predict(point.features)\n                    (point.label, prediction)\n               }.collect\n}\n\nprintln(\"== Lasso with SGD Model ==\")\nval predictionsLassoSGD = sc.parallelize(computedSetLasso.reduce(_++_))\nprintStats(predictionsLassoSGD)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : false
    },
    "cell_type" : "code",
    "source" : "// Evaluate model on test dataset\nval computedSetRidge = mapTest.map { case(idx, dataset) =>\n    dataset.map{point => \n                    val model = mapRidgeRegressionSGDModel(idx)\n                    val prediction = model.predict(point.features)\n                    (point.label, prediction)\n               }.collect\n}\n\nprintln(\"== Ridge Regression with SGD Model ==\")\nval predictionsRidgeSGD = sc.parallelize(computedSetRidge.reduce(_++_))\nprintStats(predictionsRidgeSGD)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "// Evaluate model on test instances and compute test error\nval labelsAndPredictions = test.map { point =>\n  val carrier = point.features.apply(0)\n  val model = mapDecTreeModel(carrier)\n  val prediction = model.predict(point.features)\n  (point.label, prediction)\n}\n\n\nprintln(\"== Decision Tree Model ==\")\nprintStats(labelsAndPredictions)",
    "outputs" : [ ]
  }, {
    "metadata" : {
      "trusted" : true,
      "input_collapsed" : false,
      "collapsed" : true
    },
    "cell_type" : "code",
    "source" : "",
    "outputs" : [ ]
  } ],
  "nbformat" : 4
}