[34m[INFO ][0;39m [35m[2016-07-05 13:21:23,596][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:21:24,293][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:21:24,294][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:21:24,295][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:25,011][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56674.
[34m[INFO ][0;39m [35m[2016-07-05 13:21:25,826][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56675.
[34m[INFO ][0;39m [35m[2016-07-05 13:21:25,856][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:21:25,889][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:21:25,908][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-f59536c7-b0d1-455f-bf05-bb8f261d8a67
[34m[INFO ][0;39m [35m[2016-07-05 13:21:25,931][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,021][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,443][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,446][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,613][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,650][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56676.
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,651][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56676
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,654][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,660][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56676 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56676)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:26,663][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:21:27,757][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:27,802][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:27,805][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56676 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:27,811][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,214][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,243][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,244][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,245][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,249][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,259][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,277][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,279][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,280][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56676 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,281][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,285][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,288][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,350][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,367][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,402][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,467][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,476][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 154 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,477][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,480][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,173 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,489][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,274537 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,763][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56676 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,773][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:21:28,775][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56676 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,026][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,112][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,113][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,113][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,113][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,114][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,114][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,128][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 44.9 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,130][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 60.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,131][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56676 (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,132][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,133][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,133][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,142][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,143][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,144][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,145][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,146][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,146][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,147][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,148][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,217][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,218][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,219][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,220][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 75 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,220][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 77 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:29,221][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 87 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,168][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,172][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 2027 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,172][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 2,038 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,172][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,173][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 2,060130 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,531][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56676 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,533][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:21:31,998][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,028][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,029][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56676 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,032][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,189][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,265][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,271][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,272][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,272][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,272][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,273][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,273][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,290][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,292][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,293][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56676 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,294][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,296][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,296][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,299][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,300][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,300][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,301][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,302][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,327][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,327][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,446][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,447][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,457][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,461][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,811][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 247.794186 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,826][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 10.500258 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:32,840][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 11.03083 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,053][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,053][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,053][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,054][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,056][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 756 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,057][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 756 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,058][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 760 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,058][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 759 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,058][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,761 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,059][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,059][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,060][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,061][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,062][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,063][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,069][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,072][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,073][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56676 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,074][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,075][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,075][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,083][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,088][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,089][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,090][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,091][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,092][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,092][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,103][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,117][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 9 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,120][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 10 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,120][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 10 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,128][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,128][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,173][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,173][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,174][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,176][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 87 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,177][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 87 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,178][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 92 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,213][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,215][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 136 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,216][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,217][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,138 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,218][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,952478 s
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,247][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Invoking stop() from shutdown hook
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,334][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,348][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,370][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,372][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,376][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,378][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,401][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,401][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:21:33,402][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-f489590e-838e-4519-9d77-7e659a21f8df
[34m[INFO ][0;39m [35m[2016-07-05 13:22:06,237][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:22:06,875][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:22:06,876][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:22:06,880][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:07,569][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56679.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,190][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56682.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,206][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,236][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,251][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-086d2382-fc0a-4637-b660-c9fb8ec05d24
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,274][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,344][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,615][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,618][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,721][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,742][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56683.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,743][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56683
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,745][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,748][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56683 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56683)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:08,750][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:22:09,702][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:09,765][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:09,770][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56683 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:09,780][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,102][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,120][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,121][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,122][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,133][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,151][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,155][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,156][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56683 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,157][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,160][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,162][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,210][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,221][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,247][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,324][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,332][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 147 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,335][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,341][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,165 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,352][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,249678 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,743][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,841][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,842][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,843][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,843][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,843][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,844][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,858][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 167.1 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,861][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 182.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,862][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56683 (size: 15.1 KB, free: 1140.3 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,863][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,863][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,863][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,871][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,872][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,873][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,874][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,874][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,875][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,875][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,875][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,930][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,930][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,930][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,933][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 69 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,934][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 62 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:10,935][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 63 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:11,550][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56683 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:11,555][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:22:11,556][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56683 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,597][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,607][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 1733 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,607][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,608][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 1,744 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,610][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 1,767992 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,896][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56683 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:12,897][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,368][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,400][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,401][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56683 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,403][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,559][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,620][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,624][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,625][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,625][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,626][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,626][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,627][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,642][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,644][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,645][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56683 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,645][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,647][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,647][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,649][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,650][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,651][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,651][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,652][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,652][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,652][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,652][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,669][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,670][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,670][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,670][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,738][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,739][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,739][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:13,741][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,015][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 202.62889 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,040][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 20.435461 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,057][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 13.816417 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,274][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,275][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,276][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,276][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,279][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 628 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,283][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 632 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,283][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 633 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,284][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 636 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,284][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,285][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,637 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,285][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,286][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,287][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,288][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,290][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,299][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,303][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,308][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56683 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,310][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,310][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,311][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,323][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,324][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,326][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,326][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,327][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,346][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,351][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,354][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,355][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,356][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,356][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,358][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 9 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,358][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 9 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,363][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,364][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,397][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,397][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,400][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 75 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,401][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 78 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,406][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,407][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 81 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,434][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,436][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 116 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,437][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,437][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,117 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,438][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,817719 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,603][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,612][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,621][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,622][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,623][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,625][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,627][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,628][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,631][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:22:14,632][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-68612b1c-e6ce-44a7-a844-00db0507f806
[34m[INFO ][0;39m [35m[2016-07-05 13:22:33,022][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:22:33,736][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:22:33,737][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:22:33,738][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:34,426][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56687.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,029][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56688.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,046][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,063][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,082][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-1ddf8a55-ec5f-4d9a-abed-a14976d7f995
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,099][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,198][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,444][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,446][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,559][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,578][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56689.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,578][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56689
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,579][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,582][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56689 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56689)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:35,585][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:22:36,502][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:36,607][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:36,610][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56689 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:36,615][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:22:36,995][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,009][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,010][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,010][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,012][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,022][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,037][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,041][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,042][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56689 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,043][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,048][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,050][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,106][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,117][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,147][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,250][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,263][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 182 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,265][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,268][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,201 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,280][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,284206 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,449][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56689 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,453][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,455][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56689 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,710][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,806][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,808][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,808][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,808][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,808][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,809][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,823][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 44.9 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,825][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 60.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,826][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56689 (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,827][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,827][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,828][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,835][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,835][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,836][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,837][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,837][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,837][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,838][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,838][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,887][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,890][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 54 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,890][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,892][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 56 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,902][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:37,904][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 75 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,516][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,521][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 1685 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,521][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 1,693 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,521][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,522][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 1,715082 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,799][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56689 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:39,800][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,238][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,276][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,277][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56689 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,278][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,392][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,450][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,454][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,455][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,455][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,455][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,455][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,456][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,471][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,474][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,475][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56689 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,475][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,477][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,477][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,480][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,480][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,481][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,482][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,482][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,483][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,483][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,483][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,499][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,499][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,499][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,499][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,564][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,569][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,569][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,569][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,860][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 216.365555 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,876][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 10.729341 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:40,889][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 10.802432 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,063][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,063][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,063][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,066][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 586 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,066][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 584 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,067][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 585 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,068][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,070][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 592 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,071][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,592 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,071][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,071][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,072][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,073][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,074][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,076][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,083][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,085][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,086][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56689 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,087][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,087][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,087][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,092][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,093][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,094][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,094][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,095][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,095][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,095][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,095][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,114][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,115][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,116][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,116][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,116][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,116][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,158][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,158][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,158][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,160][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 67 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,161][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 66 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,161][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 67 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,173][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,174][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,215][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,218][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 127 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,218][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,219][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,129 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,220][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,769470 s
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,377][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,387][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,396][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,396][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,398][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,400][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,404][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,404][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,407][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:22:41,408][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-ce319765-90ea-444e-8118-7d76db7c6ed8
[34m[INFO ][0;39m [35m[2016-07-05 13:35:01,505][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:35:02,077][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:35:02,078][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:35:02,079][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:02,704][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56807.
[34m[INFO ][0;39m [35m[2016-07-05 13:35:03,526][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56808.
[34m[INFO ][0;39m [35m[2016-07-05 13:35:03,570][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:35:03,602][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:35:03,624][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-6d905f3f-29fa-4629-9916-ea816bf1d868
[34m[INFO ][0;39m [35m[2016-07-05 13:35:03,647][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:35:03,743][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,123][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,127][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,284][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,318][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56809.
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,319][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56809
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,324][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,332][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56809 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56809)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:04,337][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,467][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,511][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,514][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56809 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,520][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,912][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,933][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,933][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,934][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,937][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,947][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,967][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,970][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,971][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56809 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,972][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,977][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:05,980][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,028][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,038][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,070][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,147][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,156][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 145 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,157][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,161][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,167 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,174][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,262114 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,479][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56809 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,483][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,485][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56809 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,804][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,900][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,901][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,902][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,902][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,902][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,903][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,917][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 44.9 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,920][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 60.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56809 (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,921][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,922][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,922][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,929][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,930][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,931][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,932][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,934][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,934][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,939][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:06,939][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:07,007][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:07,008][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:07,008][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:07,013][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 82 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:07,015][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 84 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:07,016][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 93 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:08,825][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:08,831][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 1898 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:08,831][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 1,908 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:08,831][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:35:08,831][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 1,931045 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,224][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56809 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,225][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,719][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,747][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,748][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56809 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,751][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,894][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,958][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,963][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,964][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,964][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,964][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,964][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,965][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,980][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,982][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,983][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56809 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,983][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,985][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,986][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,988][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,989][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,990][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,990][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,991][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,991][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,991][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:09,991][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,012][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,012][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,012][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,012][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,108][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,111][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,113][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,115][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,456][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 246.596963 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,470][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 9.528504 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,485][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 12.93545 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,690][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,690][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,690][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,690][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,693][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 704 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,694][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 705 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,694][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 704 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,695][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 708 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,695][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,695][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,709 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,696][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,697][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,698][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,699][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,700][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,710][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,712][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,714][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56809 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,714][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,715][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,715][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,721][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,723][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,723][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,724][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,725][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,725][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,742][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,742][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,755][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,756][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,756][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,760][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 12 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,761][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 12 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,760][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 10 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,768][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,770][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,809][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,810][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 87 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,812][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,813][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,815][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 93 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,818][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 94 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,854][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_4_piece0 on localhost:56809 in memory (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,869][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,871][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 152 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,872][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,873][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,154 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:10,873][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,914630 s
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,026][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,038][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,049][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,050][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,053][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,055][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,057][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,058][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,061][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:35:11,062][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-ae8db882-e9fd-4a09-a006-3c840b3a1f70
[34m[INFO ][0;39m [35m[2016-07-05 13:36:09,703][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:36:10,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:36:10,329][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:36:10,330][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,057][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56814.
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,743][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56815.
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,763][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,792][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,811][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-5d994369-d3dc-495b-98b7-08926e5472cf
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,838][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:36:11,901][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,231][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,236][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,434][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,469][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56816.
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,472][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56816
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,478][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,482][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56816 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56816)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:12,489][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:36:13,692][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:13,774][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:13,779][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56816 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:13,786][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,174][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,191][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,192][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,192][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,194][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,210][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,226][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,233][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,234][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56816 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,235][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,238][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,240][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,287][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,297][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,320][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,401][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,411][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 141 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,415][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,415][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,159 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,431][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,255777 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,813][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,964][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,968][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,969][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,969][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,969][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,970][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,985][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 167.1 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,987][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 182.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,989][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56816 (size: 15.1 KB, free: 1140.3 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,990][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,990][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,991][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,998][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:14,999][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,000][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,001][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,002][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,002][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,002][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,002][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,058][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 61 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,060][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,062][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 63 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,063][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 70 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,746][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56816 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,750][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:36:15,751][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56816 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:16,982][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:16,993][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 1992 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:16,993][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:36:16,994][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 2,003 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:16,995][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 2,030265 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:17,253][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56816 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:17,254][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:36:17,935][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,000][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,001][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56816 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,003][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,174][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,271][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,276][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,277][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,278][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,278][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,278][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,280][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,306][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,308][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,309][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56816 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,310][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,312][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,312][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,315][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,316][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,317][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,318][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,318][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,318][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,318][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,318][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,334][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,335][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,337][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,337][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,412][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,422][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,429][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,434][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,689][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 189.159122 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,703][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 9.167319 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,723][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 16.098438 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,889][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,893][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 576 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,894][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,895][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,897][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 582 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,900][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 583 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,908][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,910][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 597 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,910][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,597 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,910][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,911][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,912][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,912][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,913][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,915][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,922][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,924][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,925][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56816 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,926][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,926][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,927][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,936][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,937][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,938][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,939][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,941][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,942][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,956][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,975][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,980][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,981][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,982][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,985][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 3 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,985][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 11 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,985][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 10 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,998][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:36:18,999][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,034][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,034][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 98 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,037][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 101 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,038][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 99 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,066][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,069][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 137 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,069][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,071][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,139 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,071][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,799990 s
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,222][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,232][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,242][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,244][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,245][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,247][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,251][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,251][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,254][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:36:19,255][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-7ea213fd-9a5d-4ed7-a488-cb40c24574c8
[34m[INFO ][0;39m [35m[2016-07-05 13:41:29,561][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:41:30,034][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:41:30,035][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:41:30,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:30,707][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56829.
[34m[INFO ][0;39m [35m[2016-07-05 13:41:31,520][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56830.
[34m[INFO ][0;39m [35m[2016-07-05 13:41:31,539][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:41:31,567][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:41:31,594][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-447540cf-b9a1-4e28-aa29-645523035ad6
[34m[INFO ][0;39m [35m[2016-07-05 13:41:31,622][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:41:31,707][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,240][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,243][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,383][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,414][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56831.
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,415][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56831
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,418][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,426][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56831 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56831)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:32,429][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:41:33,641][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:33,690][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:33,693][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56831 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:33,698][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,134][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,155][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,155][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,156][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,158][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,168][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,187][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,191][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,192][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56831 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,193][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,197][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,198][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,245][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,257][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,286][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,361][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,369][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 141 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,371][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,373][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,161 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,381][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,246422 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,694][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56831 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,700][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,701][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56831 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:34,922][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,018][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,019][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,019][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,020][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 44.9 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,038][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 60.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,039][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56831 (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,040][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,041][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,041][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,049][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,050][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,051][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,052][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,054][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,056][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,059][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,059][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,141][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,141][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,143][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 93 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,144][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 95 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,148][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:35,150][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 108 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:36,922][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:36,928][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 1876 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:36,929][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 1,886 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:36,929][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:41:36,929][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 1,910639 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,316][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56831 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,317][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,769][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,795][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,795][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56831 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,796][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:41:37,943][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,020][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,025][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,025][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,025][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,026][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,026][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,027][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,042][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,043][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,045][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56831 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,045][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,047][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,047][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,049][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,050][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,050][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,051][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,051][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,051][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,051][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,051][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,082][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,084][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,093][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,093][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,190][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,194][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,195][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,203][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,535][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 248.495839 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,548][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 7.773625 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,561][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 10.707185 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,767][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,768][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,769][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,769][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,773][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 723 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,775][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 726 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,776][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 725 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,781][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 734 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,782][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,783][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,736 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,784][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,785][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,786][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,787][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,792][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,798][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,800][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,801][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56831 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,801][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,802][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,802][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,806][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,808][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,809][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,811][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,813][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,813][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,813][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,813][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,833][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,833][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,833][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,833][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,835][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,835][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,835][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,835][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 6 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,878][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,881][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,883][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,885][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 77 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,886][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 76 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,886][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 79 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,910][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,913][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 108 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,913][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,915][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,110 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:38,916][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,894785 s
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,068][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,106][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,116][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,117][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,121][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,124][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,125][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,129][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:41:39,131][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-026feb01-225a-46a1-8441-a648779aaf3f
[34m[INFO ][0;39m [35m[2016-07-05 13:42:56,153][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:42:56,877][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:42:56,878][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:42:56,879][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:42:57,579][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56834.
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,196][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56835.
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,211][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,237][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,253][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-325fecdd-1dac-4385-a31e-9642ece1edbf
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,273][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,366][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,614][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,618][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,712][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,732][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56836.
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,733][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56836
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,735][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,738][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56836 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56836)
[34m[INFO ][0;39m [35m[2016-07-05 13:42:58,741][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:42:59,689][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:42:59,759][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:42:59,765][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56836 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:42:59,774][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,103][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,119][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,120][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,122][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,131][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,153][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,156][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,158][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56836 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,158][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,162][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,164][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,207][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,217][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,255][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,338][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 150 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,340][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,341][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,166 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,350][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,246478 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,732][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,821][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,822][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,823][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,823][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,823][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,823][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,840][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 167.1 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,844][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 182.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,845][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56836 (size: 15.1 KB, free: 1140.3 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,846][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,846][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,847][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,854][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,854][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,855][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,857][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,857][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,858][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,871][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,871][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,905][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56836 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,914][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,915][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56836 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,949][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,950][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,950][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,952][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 105 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,953][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 98 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:00,953][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 98 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,635][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,640][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 1783 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,640][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 1,793 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,640][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,641][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 1,819046 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,876][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56836 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:02,877][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,295][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,326][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,327][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56836 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,457][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,519][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,523][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,524][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,524][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,524][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,524][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,525][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,542][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,544][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,545][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56836 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,546][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,547][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,548][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,550][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,551][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,551][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,552][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,553][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,553][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,553][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,553][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,571][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,575][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,575][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,576][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,644][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,647][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,653][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,653][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,957][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 233.005822 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,970][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 8.448778 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:03,982][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 9.593506 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,139][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,139][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,139][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,143][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 591 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,145][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 597 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,146][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,148][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 595 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,148][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 598 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,149][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,149][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,601 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,150][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,151][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,152][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,152][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,155][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,168][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,171][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,172][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56836 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,173][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,173][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,174][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,180][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,182][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,183][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,184][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,184][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,184][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,192][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,218][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,218][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,220][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,220][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 5 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,220][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 7 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,237][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,237][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,240][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,240][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 0 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,272][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,274][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 92 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,276][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,277][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 94 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,279][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,280][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 98 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,323][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,325][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 147 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,326][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,327][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,148 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,328][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 0,808061 s
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,475][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,486][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,495][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,496][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,497][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,499][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,503][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,504][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,507][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:43:04,508][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-66a316ea-8fc9-4144-8d67-c01e98501beb
[34m[INFO ][0;39m [35m[2016-07-05 13:44:02,929][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running Spark version 1.6.0
[34m[INFO ][0;39m [35m[2016-07-05 13:44:03,513][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing view acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:44:03,514][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Changing modify acls to: eiti
[34m[INFO ][0;39m [35m[2016-07-05 13:44:03,515][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(eiti); users with modify permissions: Set(eiti)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:04,155][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriver' on port 56839.
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,209][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'sparkDriverActorSystem' on port 56840.
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,235][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering MapOutputTracker
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,303][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering BlockManagerMaster
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,324][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created local directory at /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/blockmgr-1f8fcc8b-b637-4e92-9b3b-3a7f25fb38ef
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,354][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore started with capacity 1140.4 MB
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,529][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering OutputCommitCoordinator
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,804][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'SparkUI' on port 4040.
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,808][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started SparkUI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,940][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting executor ID driver on host localhost
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,971][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 56841.
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,972][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Server created on 56841
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,972][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Trying to register BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,976][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering block manager localhost:56841 with 1140.4 MB RAM, BlockManagerId(driver, localhost, 56841)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:05,978][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registered BlockManager
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,375][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0 stored as values in memory (estimated size 107.7 KB, free 107.7 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,441][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_0_piece0 stored as bytes in memory (estimated size 9.8 KB, free 117.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,444][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_0_piece0 in memory on localhost:56841 (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,449][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 0 from textFile at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,810][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: first at modelSaveLoad.scala:129
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,828][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 0 (first at modelSaveLoad.scala:129) with 1 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,829][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 0 (first at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,829][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,831][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,840][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,859][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1 stored as values in memory (estimated size 3.0 KB, free 120.5 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,862][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_1_piece0 stored as bytes in memory (estimated size 1777.0 B, free 122.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,863][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_1_piece0 in memory on localhost:56841 (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,865][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 1 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,869][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at textFile at modelSaveLoad.scala:129)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,871][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 0.0 with 1 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,920][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2136 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,934][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 0.0 (TID 0)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:07,964][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/metadata/part-00000:0+54
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,054][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0). 2154 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,062][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 0.0 (TID 0) in 163 ms on localhost (1/1)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,064][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 0.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,082][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 0 (first at modelSaveLoad.scala:129) finished in 0,198 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,098][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 0 finished: first at modelSaveLoad.scala:129, took 0,287553 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,629][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_1_piece0 on localhost:56841 in memory (size: 1777.0 B, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,634][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 1
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,636][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_0_piece0 on localhost:56841 in memory (size: 9.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,911][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Listing file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data on driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:08,999][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: parquet at DecisionTreeModel.scala:247
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,001][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 1 (parquet at DecisionTreeModel.scala:247) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,001][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 1 (parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,001][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,002][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List()
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,003][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,017][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2 stored as values in memory (estimated size 44.9 KB, free 44.9 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,019][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.1 KB, free 60.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,021][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_2_piece0 in memory on localhost:56841 (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,021][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 2 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,022][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at parquet at DecisionTreeModel.scala:247)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,022][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 1.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,032][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,033][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 1.0 (TID 2, localhost, partition 1,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,034][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 1.0 (TID 3, localhost, partition 2,PROCESS_LOCAL, 1995 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,035][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 1.0 (TID 4, localhost, partition 3,PROCESS_LOCAL, 2178 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 1.0 (TID 1)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 1.0 (TID 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,036][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 1.0 (TID 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,037][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 1.0 (TID 4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,102][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,102][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,103][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 1.0 (TID 2) in 71 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,104][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 1.0 (TID 1) in 80 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,105][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3). 936 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:09,107][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 1.0 (TID 3) in 74 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,427][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4). 2321 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,432][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 1.0 (TID 4) in 2397 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,433][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 1 (parquet at DecisionTreeModel.scala:247) finished in 2,409 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,433][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 1.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,433][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 1 finished: parquet at DecisionTreeModel.scala:247, took 2,433425 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,924][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed broadcast_2_piece0 on localhost:56841 in memory (size: 15.1 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:11,925][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Cleaned accumulator 2
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,461][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3 stored as values in memory (estimated size 59.6 KB, free 59.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,494][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_3_piece0 stored as bytes in memory (estimated size 13.8 KB, free 73.3 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,496][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_3_piece0 in memory on localhost:56841 (size: 13.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,498][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 3 from map at DecisionTreeModel.scala:250
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,622][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Reading Parquet file(s) from file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet, file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,684][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting job: collect at DecisionTreeModel.scala:265
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,688][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Registering RDD 7 (groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,689][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Got job 2 (collect at DecisionTreeModel.scala:265) with 4 output partitions
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,689][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Final stage: ResultStage 3 (collect at DecisionTreeModel.scala:265)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,689][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Parents of final stage: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,689][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Missing parents: List(ShuffleMapStage 2)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,690][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,706][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4 stored as values in memory (estimated size 7.4 KB, free 80.8 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,708][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 84.6 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,709][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_4_piece0 in memory on localhost:56841 (size: 3.8 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,710][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 4 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,711][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[7] at groupBy at DecisionTreeModel.scala:263)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,712][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 2.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,714][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 2.0 (TID 5, localhost, partition 0,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,715][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 2.0 (TID 6, localhost, partition 1,PROCESS_LOCAL, 2182 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,716][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 2.0 (TID 7, localhost, partition 2,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,717][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 2.0 (TID 8, localhost, partition 3,PROCESS_LOCAL, 2181 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,718][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 2.0 (TID 5)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,718][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 2.0 (TID 6)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,718][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 2.0 (TID 7)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,718][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 2.0 (TID 8)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,741][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00003-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3886 length: 3886 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,742][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00000-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3985 length: 3985 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,750][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00001-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3909 length: 3909 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,751][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Input split: ParquetInputSplit{part: file:/Users/eiti/git-repository/spark-mllib-sample/generated-models/trained-models/c1model-succ/data/part-r-00002-b59940d9-399b-4655-bd2a-bab4b5198dab.gz.parquet start: 0 end: 3887 length: 3887 hosts: []}
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,848][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,867][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,876][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:44:12,891][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Going to read the following fields from the Parquet file:

Parquet form:
message spark_schema {
  optional int32 treeId;
  optional int32 nodeId;
  optional group predict {
    optional double predict;
    optional double prob;
  }
  optional double impurity;
  optional boolean isLeaf;
  optional group split {
    optional int32 feature;
    optional double threshold;
    optional int32 featureType;
    optional group categories (LIST) {
      repeated group list {
        optional double element;
      }
    }
  }
  optional int32 leftNodeId;
  optional int32 rightNodeId;
  optional double infoGain;
}

Catalyst form:
StructType(StructField(treeId,IntegerType,true), StructField(nodeId,IntegerType,true), StructField(predict,StructType(StructField(predict,DoubleType,true), StructField(prob,DoubleType,true)),true), StructField(impurity,DoubleType,true), StructField(isLeaf,BooleanType,true), StructField(split,StructType(StructField(feature,IntegerType,true), StructField(threshold,DoubleType,true), StructField(featureType,IntegerType,true), StructField(categories,ArrayType(DoubleType,true),true)),true), StructField(leftNodeId,IntegerType,true), StructField(rightNodeId,IntegerType,true), StructField(infoGain,DoubleType,true))
       
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,323][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 323.805302 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,340][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 11.850163 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,359][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Code generated in 15.658343 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,619][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,619][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,619][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,627][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 2.0 (TID 5) in 914 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,627][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 2.0 (TID 6) in 912 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,628][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 2.0 (TID 7) in 911 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,639][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8). 2256 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,641][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 2.0 (TID 8) in 924 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,641][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ShuffleMapStage 2 (groupBy at DecisionTreeModel.scala:263) finished in 0,929 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,641][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 2.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,642][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | looking for newly runnable stages
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,643][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | running: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,644][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | waiting: Set(ResultStage 3)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,645][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | failed: Set()
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,646][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264), which has no missing parents
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,655][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 93.0 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,657][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.2 KB, free 97.2 KB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,658][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Added broadcast_5_piece0 in memory on localhost:56841 (size: 4.2 KB, free: 1140.4 MB)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,658][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Created broadcast 5 from broadcast at DAGScheduler.scala:1006
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,659][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Submitting 4 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at mapValues at DecisionTreeModel.scala:264)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,659][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Adding task set 3.0 with 4 tasks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,665][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 0.0 in stage 3.0 (TID 9, localhost, partition 0,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,667][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 1.0 in stage 3.0 (TID 10, localhost, partition 1,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,668][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 2.0 in stage 3.0 (TID 11, localhost, partition 2,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,670][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Starting task 3.0 in stage 3.0 (TID 12, localhost, partition 3,NODE_LOCAL, 1813 bytes)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,671][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 0.0 in stage 3.0 (TID 9)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,678][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 2.0 in stage 3.0 (TID 11)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,678][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 1.0 in stage 3.0 (TID 10)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,679][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Running task 3.0 in stage 3.0 (TID 12)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,688][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,692][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,695][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 10 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,696][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 4 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,697][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,698][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,698][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Getting 4 non-empty blocks out of 4 blocks
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,699][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Started 0 remote fetches in 1 ms
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,750][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,751][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,752][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 2.0 in stage 3.0 (TID 11) in 85 ms on localhost (1/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,753][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 3.0 in stage 3.0 (TID 12) in 83 ms on localhost (2/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,756][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10). 1161 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,758][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 1.0 in stage 3.0 (TID 10) in 91 ms on localhost (3/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,767][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9). 12223 bytes result sent to driver
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,770][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Finished task 0.0 in stage 3.0 (TID 9) in 106 ms on localhost (4/4)
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,771][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Removed TaskSet 3.0, whose tasks have all completed, from pool 
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,772][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | ResultStage 3 (collect at DecisionTreeModel.scala:265) finished in 0,109 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,773][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Job 2 finished: collect at DecisionTreeModel.scala:265, took 1,088862 s
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,935][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Stopped Spark web UI at http://192.168.188.6:4040
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,949][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MapOutputTrackerMasterEndpoint stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,986][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | MemoryStore cleared
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,988][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManager stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,990][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | BlockManagerMaster stopped
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,992][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | OutputCommitCoordinator stopped!
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,997][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Successfully stopped SparkContext
[34m[INFO ][0;39m [35m[2016-07-05 13:44:13,998][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | SparkContext already stopped.
[34m[INFO ][0;39m [35m[2016-07-05 13:44:14,003][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Shutdown hook called
[34m[INFO ][0;39m [35m[2016-07-05 13:44:14,005][0;39m [33m[][0;39m [35m[org.apache.spark.Logging$class->logInfo][0;39m | Deleting directory /private/var/folders/fd/s_6smjqn7mngnfdw8rz9t4w00000gp/T/spark-2f8bf933-dbdb-49f8-bc4d-87159afec315
